{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **GR5242 HW03 Problem 2: Shakespeare with LSTM networks**\n",
        "\n",
        "**Instructions**: This problem is an individual assignment -- you are to complete this problem on your own, without conferring with your classmates.  You should submit a completed and published notebook to Courseworks; no other files will be accepted.\n",
        "\n",
        "## Description:\n",
        "\n",
        "This homework exercise has 3 primary goals:\n",
        " * Introduce some basic concepts from natural language processing\n",
        " * Get some practice training recurrent neural networks, specifically on text data\n",
        " * Be able to generate fake text data from your favorite author!   \n",
        "\n",
        "By the end of this exercise, you will have a basic, but decent, computer program which can simulate the writing patterns of any author of your choice.\n",
        "\n",
        "Here is an outline of the rest of the exercise.\n",
        " 1. Data loading\n",
        "     - Downloading a text from Project Gutenberg that we will try to model\n",
        "     - Data preprocessing and numerical encoding\n",
        "     - Making a training `Dataset` object\n",
        " 3. Learn to generate text with a neural network\n",
        "     - Defining the recurrent network\n",
        "     - Training\n",
        "     - Predicting and sampling text from the model\n",
        "\n",
        "There are 12 questions (70 points) in total, which include coding and written questions. You can only modify the codes and text within \\### YOUR CODE HERE ### and/or \\### YOUR ANSWER HERE ###.\n"
      ],
      "metadata": {
        "id": "h5vtG0bbJt9u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6hwabXFdkMlH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Character-level language modeling\n",
        "\n",
        "Our goal here is to build a model of language letter-by-letter. Since we may also allow numbers, spaces, and punctuation, it's better to say character-by-character. We will start by fixing an \"alphabet\": the set of allowed characters.\n",
        "\n",
        "In math notation, let's call the alphabet $A$. In code,"
      ],
      "metadata": {
        "id": "6_-QkOsDQrgu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alphabet = \" abcdefghijklmnopqrstuvwxyz1234567890.,!?:;ABCDEFGHIJKLMNOPQRSTUVWXYZ\\n\""
      ],
      "metadata": {
        "id": "K8RdHuHPRCPJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 1: Data loading and preprocessing"
      ],
      "metadata": {
        "id": "sitlEu4BJsBm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will start by downloading training data from Project Gutenberg: https://www.gutenberg.org/. Project Gutenberg is a free repository of public domain books. Find any book you like, and download it in Plain Text UTF-8 format.\n",
        "\n",
        "For example, we will use Shakespeare's complete works: https://www.gutenberg.org/ebooks/100. There is a link on that page to the Plain Text format data.  Download the pg100.txt file, and then upload it from your computer to colab (click at left on the File icon, then click the upload icon).  \n",
        "\n",
        "*Important*: whichever work you choose, make sure you have enough data! The size of your plain text file should be at least 2MB."
      ],
      "metadata": {
        "id": "Oryq1WYLNojq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# after uploading your file to Colab, set this variable to its path:\n",
        "txt_path = \"pg100.txt\""
      ],
      "metadata": {
        "id": "NeGF0qSbJqqB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's load the text and see what it says:"
      ],
      "metadata": {
        "id": "GGNYDQKRSXBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(txt_path) as txt_file:\n",
        "  text = txt_file.read()\n",
        "\n",
        "print(\"text is\", len(text), \"characters long.\")\n",
        "print()\n",
        "print(\"A sample from the middle:\")\n",
        "print()\n",
        "print(text[len(text) // 2 : len(text) // 2 + 100])"
      ],
      "metadata": {
        "id": "7RKK_A1NShRW",
        "outputId": "d70316bb-73e7-4872-c50c-a57a9b9a4f12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text is 5546921 characters long.\n",
            "\n",
            "A sample from the middle:\n",
            "\n",
            "all bear the guilt\n",
            "Of our great quell?\n",
            "\n",
            "MACBETH.\n",
            "Bring forth men-children only;\n",
            "For thy undaunted me\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data standardization\n",
        "\n",
        "Now, we will clean the data: converting the data to lowercase, removing extra spaces and linebreaks, and get rid of characters which are not in our alphabet."
      ],
      "metadata": {
        "id": "7viKRe94P_Cx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# remove extra characters by replacing them with spaces\n",
        "text = re.sub(rf\"[^{alphabet}]\", \" \", text)"
      ],
      "metadata": {
        "id": "5m3ZqU7RS2xA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how it looks again:"
      ],
      "metadata": {
        "id": "7tgMpHS2O67G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[len(text) // 2 : len(text) // 2 + 100])"
      ],
      "metadata": {
        "id": "TiJEL-O-O6wL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49a9266e-a5d8-4666-cec5-7b8bae72d318"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all bear the guilt\n",
            "Of our great quell?\n",
            "\n",
            "MACBETH.\n",
            "Bring forth men children only;\n",
            "For thy undaunted me\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Numerical encoding\n",
        "\n",
        "Unfortunately, neural networks don't understand text. So, we need to convert our characters to numerical values. Here are some helper functions for doing this."
      ],
      "metadata": {
        "id": "8JFrb5Jr92F8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's build a dictionary mapping characters to integers\n",
        "char2int = {c: i for i, c in enumerate(alphabet)}\n",
        "alphabet_array = np.array([c for c in alphabet])\n",
        "\n",
        "# this function will turn a string into a numpy array of integers\n",
        "def int_encode(string):\n",
        "  if any(c not in char2int for c in string):\n",
        "    raise ValueError(\n",
        "        \"Found a character which was not in the alphabet in the input \"\n",
        "        f\"to int_encode. Valid alphabet characters: {alphabet}\"\n",
        "      )\n",
        "  return np.array([char2int[c] for c in string])\n",
        "\n",
        "# this function will decode a numpy array of integers back to a string\n",
        "def int_decode(int_array):\n",
        "  return ''.join(alphabet_array[int_array])"
      ],
      "metadata": {
        "id": "1LjvsaNy91to"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Question 1a: 4 points) Test out `int_encode` by passing `test_string` in and printing the result."
      ],
      "metadata": {
        "id": "LsSPJalSWFUm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's test these out!\n",
        "### YOUR CODE HERE ###\n",
        "test_string=text[len(text) // 2 : len(text) // 2 + 100] \n",
        "int_encode(test_string)"
      ],
      "metadata": {
        "id": "vU8Umqsw-4CN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ae2aae8-6169-4be5-df15-7bc03b3f76f4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1, 12, 12,  0,  2,  5,  1, 18,  0, 20,  8,  5,  0,  7, 21,  9, 12,\n",
              "       20, 69, 57,  6,  0, 15, 21, 18,  0,  7, 18,  5,  1, 20,  0, 17, 21,\n",
              "        5, 12, 12, 40, 69, 69, 55, 43, 45, 44, 47, 62, 50, 37, 69, 44, 18,\n",
              "        9, 14,  7,  0,  6, 15, 18, 20,  8,  0, 13,  5, 14,  0,  3,  8,  9,\n",
              "       12,  4, 18,  5, 14,  0, 15, 14, 12, 25, 42, 69, 48, 15, 18,  0, 20,\n",
              "        8, 25,  0, 21, 14,  4,  1, 21, 14, 20,  5,  4,  0, 13,  5])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Question 1b: 4 points) Decode the result from the last cell using `int_decode` to make sure it is the same as `test_string`"
      ],
      "metadata": {
        "id": "wGFYXZPcWL8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###\n",
        "int_decode(int_encode(test_string))"
      ],
      "metadata": {
        "id": "392dWPKn0J85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "739b61c4-c0a4-4c36-d4a6-867748c5f690"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'all bear the guilt\\nOf our great quell?\\n\\nMACBETH.\\nBring forth men children only;\\nFor thy undaunted me'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Is the decoding the same as `test_string`? It should -- you have a bug above if not."
      ],
      "metadata": {
        "id": "SWxJykne_VKY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make a training dataset\n",
        "\n",
        "First, we make a numerical encoded version of the entire dataset:"
      ],
      "metadata": {
        "id": "oIqRJLtyAC--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enctext = int_encode(text)"
      ],
      "metadata": {
        "id": "JOr_T3ddALvQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use `tf.convert_to_tensor` to make it into a TensorFlow tensor:"
      ],
      "metadata": {
        "id": "j1FLc9MABXKu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enctext = tf.convert_to_tensor(enctext)"
      ],
      "metadata": {
        "id": "ORDJL4_0BWTK"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, convert to a tensorflow `Dataset` using `tf.data.Dataset.from_tensor_slices`:"
      ],
      "metadata": {
        "id": "VDmg1fVsBALW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_dataset = tf.data.Dataset.from_tensor_slices(enctext)\n",
        "text_dataset"
      ],
      "metadata": {
        "id": "rXIvdUjaAPVc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6253e6d6-d82d-45a5-9cb9-4b170413fad2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 2: Training a NN\n",
        "\n",
        "Our model will work as follows:\n",
        " - One-hot encoded input gets passed into a linear embedding layer. These two operations are combined with the `Embedding` layer: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding\n",
        " - LSTM cell\n",
        " - Linear decoder layer\n",
        "\n",
        "TensorFlow/Keras has two main ways of interfacing with recurrent networks. In the case of LSTMs, those are:\n",
        " - the LSTM layer https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM\n",
        " - the LSTMCell layer https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTMCell\n",
        "\n",
        "Both models are sequential: the goal is to process a batch of sequences of input features and produce a batch of sequences of output features. The `LSTM` class makes this simple and easy, and the `LSTMCell` class gives more control by allowing you to process the sequences one element at a time. We will use the `LSTM` layer to keep things simple, but keep in mind that some of what we do could be made more efficient with `LSTMCell`.\n",
        "\n",
        "The inputs and outputs to recurrent networks in Keras have shape: `(batch_dimension, sequence_dimension, feature_dimension)`. In this case, our feature dimension is `len(alphabet)`.\n",
        "\n",
        "Something to keep in mind: the output of this network will be stateful! In each batch, the `k`th output along the sequence dimension will be the logits for predicting the `k+1`th input in the batch."
      ],
      "metadata": {
        "id": "Pk3z0bACJyQc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We will use this constant below\n",
        "HIDDEN_DIM = 128"
      ],
      "metadata": {
        "id": "F80hsi-FH4WA"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Question 2a: 10 points) Model definition: make a Sequential model with an Embedding layer with input dimension `len(alphabet)` and output dimension `HIDDEN_DIM`, followed by an LSTM layer with `HIDDEN_DIM` features, followed by a Dense layer with `len(alphabet)` features"
      ],
      "metadata": {
        "id": "d2syDhfkWdXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(len(alphabet), HIDDEN_DIM))\n",
        "model.add(tf.keras.layers.LSTM(HIDDEN_DIM, return_sequences=True))\n",
        "model.add(tf.keras.layers.Dense(len(alphabet))) # YOUR CODE HERE\n",
        "\n",
        "model.summary"
      ],
      "metadata": {
        "id": "REnQC0MXiyAL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87a98bf8-9adc-4317-b21a-8acbb2d2e24f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Model.summary of <keras.engine.sequential.Sequential object at 0x7fe3e3e58c90>>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Question 2b: 8 points) If we want to use the output of the model as logits for predicting a character (which we can think of as a class), what loss should we use?"
      ],
      "metadata": {
        "id": "fzp4Q7vFJXVj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss =tf.keras.losses.SparseCategoricalCrossentropy() # YOUR CODE HERE"
      ],
      "metadata": {
        "id": "u1XFATDzieB5"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=\"adam\", loss=loss)"
      ],
      "metadata": {
        "id": "RsZoPv7wUilY"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining some parameters about data batching, explained in the next section\n",
        "# Note: after you get the entire assignment working, you can make these bigger and train for longer, to get better performance\n",
        "SEQUENCE_LENGTH = 32\n",
        "BATCH_SIZE = 16"
      ],
      "metadata": {
        "id": "_ZUt-1--LP32"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Making the dataset of (input, target) pairs\n",
        "\n",
        "To train the model, we need to make a `tf.data.Dataset` containing input and target sequences. Our input sequences will be sequences of length `SEQUENCE_LENGTH` containing int-encoded characters from the input. Our target sequences will be the \"next characters\" corresponding to the input sequence: so, if the input sequence is the 10th, 11th, ... characters, then the target sequence is the 11th, 12th, ... characters.\n",
        "\n",
        "We will walk through using `tf.data.Dataset` methods to create these."
      ],
      "metadata": {
        "id": "h2ORoPzNJkKP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Question 2c: 8 points) Use our friend the `batch` method of `text_dataset`, which we defined above, to make sequences of length `SEQUENCE_LENGTH`."
      ],
      "metadata": {
        "id": "7PlSDJOjL_Fb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_seqs = text_dataset.batch(SEQUENCE_LENGTH) # YOUR CODE HERE\n",
        "input_seqs"
      ],
      "metadata": {
        "id": "dX6fJc84W7Lo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f66d9044-d5f1-4715-d2b9-dd2f451adece"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset element_spec=TensorSpec(shape=(None,), dtype=tf.int64, name=None)>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Question 2d: 8 points) Now, use batch again to create target sequences from the following version of the dataset which has been offset by 1 element:"
      ],
      "metadata": {
        "id": "n24Q5ySjU2ed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_text_dataset = text_dataset.skip(1)\n",
        "target_seqs = target_text_dataset.batch(SEQUENCE_LENGTH) # YOUR CODE HERE"
      ],
      "metadata": {
        "id": "BkjMZfS3XAae"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Question 2e: 6 points) Now, use the function `tf.data.Dataset.zip` to create a dataset of (input, target) pairs:"
      ],
      "metadata": {
        "id": "geD7pQ7TVH1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pairs = tf.data.Dataset.zip((input_seqs,target_seqs)) # YOUR CODE HERE\n",
        "pairs"
      ],
      "metadata": {
        "id": "B5rrw7OLXGjS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14ad2fa1-5256-45e9-80ce-aac12daa9ef9"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<ZipDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.int64, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Question 2f: 4 points) Finally, call `.batch` again to make batches of pairs of length `BATCH_SIZE`:"
      ],
      "metadata": {
        "id": "JX1_4b0oVtdv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pairs_batched = pairs.batch(BATCH_SIZE) # YOUR CODE HERE\n",
        "pairs_batched"
      ],
      "metadata": {
        "id": "pE547bYpXJge",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07d2406e-dc73-4ec7-fad9-170461b369a9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset element_spec=(TensorSpec(shape=(None, None), dtype=tf.int64, name=None), TensorSpec(shape=(None, None), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This fixes the size of the training data\n",
        "# 8000 batches is a reasonable starting number.\n",
        "train = pairs_batched.shuffle(1000).take(8000) "
      ],
      "metadata": {
        "id": "7IXwT0d9cSJW"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Question 2g: 2 points) Train the model! \n",
        "Note:\n",
        "\n",
        "\n",
        "1.   Given the above parameters, this training should take about 5 minutes.  \n",
        "2.   Performance will improve throughout training but will not get very good (as judged by the samples).  For this exercise, that is adequate.  \n",
        "3. However, to really see what this model can do, you can (should!) set a larger number of batches above and a larger hidden state so that you can take many epochs of the data.  If you train for ~10hrs on a model with 256 hidden states (as we show in class), performance can be quite good.  This step is not required for the assignment, but please do try it on your own.\n",
        "\n"
      ],
      "metadata": {
        "id": "Psw5Vbpk3J3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###\n",
        "model.fit(train, epochs=1, batch_size=512)"
      ],
      "metadata": {
        "id": "wrOmFS_eV21G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d5ae916-fbec-413d-c3bf-86f425070034"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8000/8000 [==============================] - 347s 43ms/step - loss: 2.8853\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe3dfa8fed0>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, make sure the loss goes down as it trains."
      ],
      "metadata": {
        "id": "1WbYcgKmj1zL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 3: Did it work? Let's see what the model learned\n",
        "\n",
        "Here, we'll write some functions to see how well the model has learned to predict text and to draw samples from the model.\n",
        "\n",
        "First, we'll give you a function to \"seed\" the model with some input text and then predict the most likely future text. It will be your job to create a variation on this function in the question below, so make sure you understand how it works."
      ],
      "metadata": {
        "id": "rmysazCNc14W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(seed_string, sample_length=50):\n",
        "  # Convert seed_string to int\n",
        "  current_text_ints = list(int_encode(seed_string))\n",
        "\n",
        "  for i in range(sample_length):\n",
        "    # Add an empty batch dimension and convert to tensor\n",
        "    text_arr = np.array(current_text_ints).reshape(1, -1)\n",
        "    text_arr = tf.convert_to_tensor(text_arr)\n",
        "\n",
        "    # Get the full sequence of predictions, remove the batch dim\n",
        "    logits = model(text_arr)[0]\n",
        "\n",
        "    # Remove the batch dimension and get the final logits\n",
        "    final_logits = logits[-1]\n",
        "\n",
        "    # Get the prediction using tf.argmax\n",
        "    pred = tf.argmax(final_logits)\n",
        "\n",
        "    # Append this to `current_text_ints`\n",
        "    current_text_ints.append(pred.numpy())\n",
        "  \n",
        "  return int_decode(np.array(current_text_ints))"
      ],
      "metadata": {
        "id": "UUaB5I6VXTdi"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_seed = \"to be, or \"\n",
        "predict(test_seed, 50)"
      ],
      "metadata": {
        "id": "90ht1OAtegjv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7121b2a0-0a93-40f6-c49b-8b977076a92c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'to be, or the the the the the the the the the the the the th'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# feel free to try your own seed!  "
      ],
      "metadata": {
        "id": "1gT8Oi_KgocR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems like maybe the model learned something, but the output is a little boring. Let's make it more interesting with *randomness*!\n",
        "\n",
        "Right now, the function always picks the most likely next letter. Instead, let's sample the next letter from the model's predicted probability distribution.\n",
        "\n",
        "(Question 3a: 8 points) Fill in the blanks in the function below."
      ],
      "metadata": {
        "id": "c5f92OC-gORk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(seed_string, sample_length=50):\n",
        "  # Convert seed_string to int\n",
        "  current_text_ints = list(int_encode(seed_string))\n",
        "\n",
        "  for i in range(sample_length):\n",
        "    # Add an empty batch dimension and convert to tensor\n",
        "    text_arr = np.array(current_text_ints).reshape(1, -1)\n",
        "    text_arr = tf.convert_to_tensor(text_arr)\n",
        "\n",
        "    # Get the full sequence of predictions, remove the batch dim\n",
        "    logits = model(text_arr)[0]\n",
        "\n",
        "    # Remove the batch dimension and get the final logits\n",
        "    final_logits = logits[-1]\n",
        "\n",
        "    # Normalize the final_logits to a probability distribution\n",
        "    probs = tf.nn.softmax(final_logits)  # YOUR CODE HERE\n",
        "\n",
        "    # Call .numpy so we can use a numpy function\n",
        "    probs = probs.numpy()\n",
        "\n",
        "    # Sample from the probability distribution using\n",
        "    # the function np.random.choice\n",
        "    index=np.arange(len(probs))\n",
        "    sample = int(np.random.choice(index,1,p=probs))  # YOUR CODE HERE\n",
        "\n",
        "    # Append this to `current_text_ints`\n",
        "    current_text_ints.append(sample)\n",
        "  \n",
        "  return int_decode(np.array(current_text_ints))"
      ],
      "metadata": {
        "id": "m4Oo888KiQyS"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Question 3b: 6 points) Test this function `generate`. Is its output different from `predict` when given the same seed? How does it differ, and why?"
      ],
      "metadata": {
        "id": "Kp07ZVlUhtpy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE\n",
        "generate(test_seed, 50)"
      ],
      "metadata": {
        "id": "d75C0gswiOU2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c71ad196-efc2-4d45-d9b0-cfdf2dd495f5"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'to be, or BGX; 8ouf!BT;qGev? m6P8UC8BK.F\\nbjtt6buUs8ct5dsEH?q'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is different from the output from the predict function with more random characters. It is because we take randomness from the alphabet and each character has a chance to be shown after the test string so that the output will have nonsense to read.  "
      ],
      "metadata": {
        "id": "H6xK6qqD1ieK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Question 3c: 2 points) Try running `generate` a few times with the same seed. Are the results the same or different? Why?"
      ],
      "metadata": {
        "id": "Fy3J9i71hnWT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " '### Your Answer Here ###'<br>\n",
        " The results are not the same. It is because we take the sample with normal distribution of probs index to predict the text. Everything comes after the test string will be randomly chosen from the sample and build a text with random characters."
      ],
      "metadata": {
        "id": "YEhXk1EnOIKd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EKfPiOWvOL-s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}